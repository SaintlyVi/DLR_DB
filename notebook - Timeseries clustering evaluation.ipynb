{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Load Profile Timeseries Clustering Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "from math import ceil\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.offline as po\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tools\n",
    "import colorlover as cl\n",
    "#from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "\n",
    "import matplotlib. pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import evaluation.eval_clusters as ec\n",
    "import evaluation.eval_cluster_plot as pc\n",
    "from support import data_dir\n",
    "eval_dir = os.path.join(data_dir,'cluster_evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = ec.getExperiments('exp')\n",
    "exp2 = ec.getExperiments('exp2')\n",
    "exp3 = ec.getExperiments('exp3')\n",
    "exp4 = ec.getExperiments('exp4')\n",
    "exp5 = ec.getExperiments('exp5')\n",
    "exp6 = ec.getExperiments('exp6')\n",
    "#experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Cluster Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davies-Bouldin Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterIndex('dbi', 'Davies-Bouldin Index', experiments, groupby='algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Index Adequacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterIndex('mia','Mean Index Adequacy', experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score\n",
    "\n",
    "The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterIndex('silhouette', 'Silhouette Score', experiments, groupby='algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Cluster Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterIndex('score','Combined Cluster Score', experiments, groupby='algorithm', ylog=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Cluster Labels, Centroids and Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select best clusters for different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_results = ec.readResults()\n",
    "\n",
    "selected_clusters = dict()\n",
    "for e in experiments:\n",
    "    clusters = ec.selectClusters(cluster_results, 5, threshold=1200, experiment=e)\n",
    "    selected_clusters[e] = clusters\n",
    "selected_clusters['best'] = ec.selectClusters(cluster_results, 10, threshold=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_clusters['best']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Cluster Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_exp = ['exp2_kmeans_unit_norm', 'exp4_kmeans_unit_norm', 'exp5_kmeans_unit_norm', 'exp6_kmeans_unit_norm',\n",
    "            'exp6_kmeans_demin']\n",
    "            #'exp4_kmeans_zero-one', 'exp5_kmeans_zero-one']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = dict()\n",
    "for e in best_exp[-1:]:\n",
    "    labels = ec.getLabels(e)\n",
    "    cluster_labels[e] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get denormalised (real) cluster centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_cluster_centroids = dict()\n",
    "\n",
    "for e in best_exp:\n",
    "    rccpath = os.path.join(eval_dir, 'best_centroids', e +'_centroids.csv')\n",
    "    centroids  = pd.read_csv(rccpath, index_col='k')\n",
    "    real_cluster_centroids[e] = centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get normalised cluster centroids \n",
    "#### only for exp2 & exp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_cluster_centroids = dict()\n",
    "for k, v in selected_clusters.items():\n",
    "    name = k\n",
    "    centroids, cs, meta  = ec.getCentroids(v)\n",
    "    norm_cluster_centroids[name] = {'centroids':centroids, 'cluster_size':cs, 'description':meta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_cluster_centroids['exp2_kmeans_unit_norm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec.exploreAMDBins(cluster_results, 'exp5_kmeans_unit_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec.exploreAMDBins(cluster_results, 'exp6_kmeans_unit_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "pc.plotClusterCentroids(real_cluster_centroids[best_exp[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Patterns in Cluster Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Cluster Label Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterLabels(list(cluster_labels.values())[2], 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise TEMPORAL Cluster Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pc.plotClusterSpecificity(list(cluster_labels.values())[1], corr_list=['daytype','weekday','monthly','season','yearly'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise CONTEXTUAL Cluster Specificity (Daily Demand Assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "int100_likelihood, q100_likelihood = ec.demandCorr(list(cluster_labels.values())[1], compare='total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Equally spaced daily demand intervals\n",
    "i = int100_likelihood.stack().reset_index()\n",
    "i.columns = ['int100_bins', 'cluster', 'values']\n",
    "fig = i.iplot(kind='heatmap', x = 'int100_bins', y='cluster', z='values', colorscale='Reds', \n",
    "              title= 'Heatmap of relative likelihood of Cluster k being used in consumption bin', asFigure=True)\n",
    "fig['layout']['xaxis'].update(dict(title = 'total daily demand bins (Amps)', \n",
    "                                   tickmode='array', tickvals=list(range(0,100,10)), ticktext = list(range(0,1000,100))))\n",
    "fig['layout']['yaxis'].update(dict(title='Cluster k'))\n",
    "po.iplot(fig)\n",
    "\n",
    "#Equally sized daily demand intervals (quantiles)\n",
    "rel_q100 = q100_likelihood.drop(columns='Cluster 33')/0.01\n",
    "\n",
    "slatered=['#232c2e', '#ffffff','#c34513']\n",
    "label_cmap, label_cs = pc.colorscale_from_list(slatered, 'label_cmap') \n",
    "colorscl= pc.asymmetric_colorscale(rel_q100, label_cmap, ref_point=1.0)\n",
    "\n",
    "heatmap = go.Heatmap(z = rel_q100.T.values, x = rel_q100.index, y = rel_q100.columns, name = 'corr', \n",
    "                          colorscale=colorscl)\n",
    "layout = go.Layout(\n",
    "        title= 'Heatmap of relative likelihood of Cluster k being used in consumption quantile',\n",
    "        xaxis=dict(title = 'total daily demand quantiles (Amps) - log scale', type='log'),\n",
    "        yaxis=dict(title ='Cluster k'))\n",
    "fig = {'data':[heatmap], 'layout':layout }\n",
    "po.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Cluster Representativity and Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumption Error - total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_consE = dict()\n",
    "cepath = os.path.join(eval_dir, 'consumption_error.csv')\n",
    "consumption_error = pd.read_csv(cepath, index_col='k', usecols=['k','experiment','compare',\n",
    "                                                                'mape','mdape','mdlq','mdsyma']).drop_duplicates()\n",
    "consumption_error.rename({'experiment':'experiment_name'}, axis=1)\n",
    "\n",
    "for e in best_exp:\n",
    "    consE = consumption_error.loc[(consumption_error.experiment==e)&(consumption_error.compare=='total'),:]\n",
    "    total_consE[e] = {'mape':consE.mape,'mdape':consE.mdape,'mdlq':consE.mdlq,'mdsyma':consE.mdsyma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterMetrics(total_consE, 'TOTAL consumption error evaluation metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumption Error - max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_consE = dict()\n",
    "for e in best_exp:\n",
    "    consE = consumption_error.loc[(consumption_error.experiment==e)&(consumption_error.compare=='peak'),:]\n",
    "    peak_consE[e] = {'mape':consE.mape,'mdape':consE.mdape,'mdlq':consE.mdlq,'mdsyma':consE.mdsyma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterMetrics(peak_consE, 'PEAK consumption error evaluation metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak Coincidence Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_coincR = dict()\n",
    "pcrpath = os.path.join(eval_dir, 'peak_coincidence.csv')\n",
    "peak_eval = pd.read_csv(pcrpath, index_col='k').drop_duplicates()\n",
    "\n",
    "for e in best_exp:\n",
    "    peak_coincR[e] = {'coincidence_ratio': peak_eval.loc[peak_eval['experiment']==e,'coincidence_ratio']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterMetrics(peak_coincR, 'daily peak coincidence ratios', metric='coincidence_ratio', make_area_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Entropy - TEMPORAL\n",
    "#### weekday, month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_entropy = dict()\n",
    "max_entropy = dict()\n",
    "\n",
    "for k,l in cluster_labels.items():\n",
    "    weekday_likelihood, relative_likelihood = ec.weekdayCorr(l)\n",
    "    monthly_likelihood, relative_likelihood = ec.monthlyCorr(l)\n",
    "    \n",
    "    wce, wme = ec.clusterEntropy(weekday_likelihood, random_likelihood=None)\n",
    "    mce, mme = ec.clusterEntropy(monthly_likelihood, random_likelihood=None)\n",
    "    \n",
    "    temporal_entropy[k] = {'weekday_entropy':wce.reset_index(drop=True),'monthly_entropy':mce.reset_index(drop=True)}\n",
    "    max_entropy[k] = {'weekday_entropy':wme,'monthly_entropy':mme}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterMetrics(temporal_entropy, 'time-derived cluster entropy')#, metric='weekday_entropy', make_area_plot=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Entropy - CONTEXTUAL\n",
    "#### total daily demand, max daily demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_entropy = dict()\n",
    "\n",
    "for k,l in cluster_labels.items():\n",
    "    total_int, total_q = ec.demandCorr(l, compare='total')\n",
    "    peak_int, peak_q = ec.demandCorr(l, compare='peak')\n",
    "   \n",
    "    ice, ime = ec.clusterEntropy(total_int, random_likelihood=None)\n",
    "    pce, pme = ec.clusterEntropy(peak_int, random_likelihood=None)\n",
    "    \n",
    "    context_entropy[k] = {'total_entropy':ice.reset_index(drop=True),'peak_entropy':pce.reset_index(drop=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterMetrics(context_entropy, 'context-derived cluster entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = ['total_consE', 'peak_consE', 'peak_coincR', 'temporal_entropy', 'context_entropy']\n",
    "mean_measures = list()\n",
    "\n",
    "for m in measures:\n",
    "    m_data = eval(m)\n",
    "    for k,v in m_data.items():\n",
    "        for i,j in v.items():\n",
    "            me = ec.meanError(j)\n",
    "            mean_measures.append([m, k, i, me])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_table = pd.DataFrame(mean_measures, columns=['measure','experiment','metric','value'])\n",
    "evalcrit = evaluation_table.measure.apply(lambda x: x.split('_',1)[1])\n",
    "evaluation_table.insert(0, 'evaluation_criteria', evalcrit)\n",
    "evaluation_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dict = []\n",
    "for index, row in evaluation_table.iterrows(): \n",
    "    r = dict(zip(list(evaluation_table.columns),row.values))\n",
    "    mean_dict.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#et = evaluation_table.set_index(['evaluation_criteria','experiment','measure','metric']).unstack(level=['evaluation_criteria','measure','metric'])\n",
    "et = evaluation_table.set_index(['evaluation_criteria','experiment','measure','metric']).unstack(level=['experiment'])\n",
    "et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
