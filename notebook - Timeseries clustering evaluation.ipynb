{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Load Profile Timeseries Clustering Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "from math import ceil\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.offline as po\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tools\n",
    "import colorlover as cl\n",
    "#from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "\n",
    "import matplotlib. pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import evaluation.eval_clusters as ec\n",
    "import evaluation.eval_cluster_plot as pc\n",
    "from support import data_dir, image_dir\n",
    "eval_dir = os.path.join(data_dir,'cluster_evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = ec.getExperiments('exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Cluster Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davies-Bouldin Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterIndex('dbi', 'Davies-Bouldin Index', experiments, groupby='algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Index Adequacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterIndex('mia','Mean Index Adequacy', experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score\n",
    "\n",
    "The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterIndex('silhouette', 'Silhouette Score', experiments, groupby='experiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Cluster Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterIndex('score','Combined Cluster Score', experiments, groupby='algorithm', ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterIndex('score','Combined Cluster Score', experiments, groupby='experiment', ylog=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Cluster Labels, Centroids and Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select best clusters for different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_results = ec.readResults()\n",
    "\n",
    "selected_clusters = ec.selectClusters(cluster_results, len(cluster_results))\n",
    "selected_clusters.head(10).set_axis(range(1,11), inplace=False)\n",
    "#selected_clusters = dict()\n",
    "#for e in experiments:\n",
    "#    clusters = ec.selectClusters(cluster_results, 5, experiment=e)\n",
    "#    selected_clusters[e] = clusters\n",
    "#selected_clusters['best'] = ec.selectClusters(cluster_results, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "x0 = selected_clusters[selected_clusters.pre_processing\t.isna()]['score']  \n",
    "x1 = selected_clusters[selected_clusters.pre_processing\t=='unit_norm']['score']  \n",
    "x2 = selected_clusters[selected_clusters.pre_processing=='demin']['score']\n",
    "x3 = selected_clusters[selected_clusters.pre_processing=='zero-one']['score'] \n",
    "x4 = selected_clusters[selected_clusters.pre_processing=='sa_norm']['score']\n",
    "\n",
    "# Group data together\n",
    "hist_data = [x0, x1, x2, x3, x4]\n",
    "\n",
    "group_labels = ['no norm', 'unit norm', 'demin', 'zero-one', 'SA norm']\n",
    "\n",
    "# Create distplot with custom bin_size\n",
    "fig = ff.create_distplot(hist_data, group_labels, histnorm='percent', bin_size=1, \n",
    "                         show_curve=False, show_rug=False)\n",
    "fig['layout'].update(title='Distribution of Quantitative Scores across Normalisation Algorithms', \n",
    "                     xaxis = dict(title='bins for score range 0-100', range=[0, 100]), \n",
    "                     yaxis = dict(title='Percent'),\n",
    "                     margin=dict(t=30),\n",
    "                     height=250, width=600)\n",
    "\n",
    "# Plot!\n",
    "po.iplot(fig)\n",
    "po.plot(fig, filename=image_dir+'/clustering_evaluation/DistplotQuantScoresNormalisation'+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "y0 = selected_clusters[selected_clusters.experiment_name.str.contains('exp1|exp2|exp3')]['score']  \n",
    "y1 = selected_clusters[selected_clusters.experiment_name.str.contains('exp4|exp5|exp6')]['score']  \n",
    "y2 = selected_clusters[selected_clusters.experiment_name.str.contains('exp7|exp8')]['score']\n",
    "\n",
    "# Group data together\n",
    "hist_data2 = [y0, y1, y2]\n",
    "\n",
    "group_labels2 = ['no pre-binning', 'AMC', 'integral kmeans']\n",
    "\n",
    "fig2 = ff.create_distplot(hist_data2, group_labels2, histnorm='percent', bin_size=1, \n",
    "                          show_curve=False, show_rug=False)\n",
    "fig2['layout'].update(title='Distribution of Quantitative Scores across Pre-binning Algorithms', \n",
    "                     xaxis = dict(title='bins for score range 0-100', range=[0, 100]), \n",
    "                     yaxis = dict(title='Percent'),\n",
    "                     margin=dict(t=30),\n",
    "                     height=250, width=600)\n",
    "# Plot!\n",
    "po.iplot(fig2)\n",
    "po.plot(fig2, filename=image_dir+'/clustering_evaluation/DistplotQuantScoresPrebinning'+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "z0 = selected_clusters[selected_clusters.algorithm=='kmeans']['score']  \n",
    "z1 = selected_clusters[selected_clusters.algorithm=='som']['score']  \n",
    "z2 = selected_clusters[selected_clusters.algorithm=='som+kmeans']['score']\n",
    "\n",
    "# Group data together\n",
    "hist_data3 = [z0, z1, z2]\n",
    "\n",
    "group_labels3 = ['kmeans', 'som', 'som+kmeans']\n",
    "\n",
    "fig3 = ff.create_distplot(hist_data3, group_labels3, histnorm='percent', bin_size=1, \n",
    "                          show_curve=False, show_rug=False)\n",
    "fig3['layout'].update(title='Distribution of Quantitative Scores across Clustering Algorithms', \n",
    "                     xaxis = dict(title='bins for score range 0-100', range=[0, 100]), \n",
    "                     yaxis = dict(title='Percent'),\n",
    "                     margin=dict(t=30),\n",
    "                     height=250, width=600)\n",
    "# Plot!\n",
    "po.iplot(fig3)\n",
    "#po.plot(fig3, filename=image_dir+'/clustering_evaluation/DistplotQuantScoresClustering'+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_exp = ['exp2_kmeans_unit_norm', 'exp5_kmeans_zero-one', 'exp4_kmeans_zero-one', 'exp6_kmeans_unit_norm',\n",
    "            'exp5_kmeans_unit_norm','exp7_kmeans_unit_norm','exp8_kmeans_unit_norm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get denormalised (real) cluster centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_cluster_centroids = dict()\n",
    "\n",
    "for e in best_exp:\n",
    "    rccpath = os.path.join(eval_dir, 'best_centroids', e +'BEST1_centroids.csv')\n",
    "    centroids  = pd.read_csv(rccpath, index_col='k')\n",
    "    real_cluster_centroids[e] = centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 6\n",
    "ex = ec.exploreAMDBins(best_exp[i])\n",
    "mapper = ec.mapBins(real_cluster_centroids[best_exp[i]])\n",
    "ex['elec_bin'] = ex['elec_bin'].apply(lambda x:mapper[x])\n",
    "ex['elec_bin'] = ex['elec_bin'].astype('category')\n",
    "ex.elec_bin.cat.reorder_categories(mapper.values(), ordered=True,inplace=True)\n",
    "ex.sort_values(['elec_bin'], inplace=True)\n",
    "ex.reset_index().drop(columns=['som_dim'],axis=0).set_index(['experiment_name','elec_bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_cluster_centroids['exp8_kmeans_unit_norm'].loc[55, 'cluster_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,7):\n",
    "    pc.plotClusterCentroids(real_cluster_centroids[best_exp[i]], groupby=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Patterns in Cluster Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise TEMPORAL Cluster Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6,7):\n",
    "    pc.plotClusterSpecificity(best_exp[i], corr_list=['daytype','weekday'], threshold=1200, relative=[[5,1,1],1])\n",
    "    pc.plotClusterSpecificity(best_exp[i], corr_list=['season','monthly'], threshold=1200, relative=[[8, 4],1])\n",
    "    pc.plotClusterSpecificity(best_exp[i], corr_list=['yearly'], threshold=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise CONTEXTUAL Cluster Specificity (Daily Demand Assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment = 'exp8_kmeans_unit_norm'\n",
    "\n",
    "corr_path = os.path.join(data_dir, 'cluster_evaluation', 'k_correlations')\n",
    "\n",
    "dif = pd.read_csv(os.path.join(corr_path, 'demandi_corr.csv'), index_col=[0,1,2], header=[0]).drop_duplicates()\n",
    "dif_temp = dif.reset_index(level=[-2,-1])\n",
    "int100_total = dif_temp[(dif_temp.experiment==experiment+'BEST1')&(dif_temp.compare=='total')].drop(['experiment','compare'],axis=1)\n",
    "\n",
    "dqf = pd.read_csv(os.path.join(corr_path, 'demandq_corr.csv'), index_col=[0,1,2], header=[0]).drop_duplicates()\n",
    "dqf_temp = dqf.reset_index(level=[-2,-1])\n",
    "q100_total = dqf_temp[(dqf_temp.experiment==experiment+'BEST1')&(dqf_temp.compare=='total')].drop(['experiment','compare'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Equally spaced daily demand intervals\n",
    "i = int100_total.T.stack().reset_index()\n",
    "i.columns = ['int100_bins', 'cluster', 'values']\n",
    "\n",
    "heatmap = go.Heatmap(z = i['values'], x = i['int100_bins'], y = i['cluster'], \n",
    "                          colorscale='Reds')\n",
    "layout = go.Layout(\n",
    "        title= 'Relative likelihood that cluster k is used in particular consumption bin',\n",
    "        xaxis=dict(title = 'total daily demand bins (Amps)', \n",
    "                   tickmode='array', tickvals=list(range(0,100,10)), ticktext = list(range(0,1000,100))),\n",
    "        yaxis=dict(title ='k clusters for '+experiment)\n",
    "        )\n",
    "fig = {'data':[heatmap], 'layout':layout }\n",
    "po.iplot(fig)\n",
    "\n",
    "#Equally sized daily demand intervals (quantiles)\n",
    "rel_q100 = q100_total.T[1::]#.drop(columns=37)/0.01\n",
    "\n",
    "slatered=['#232c2e', '#ffffe0','#c34513']\n",
    "label_cmap, label_cs = pc.colorscale_from_list(slatered, 'label_cmap') \n",
    "colorscl= pc.asymmetric_colorscale(rel_q100, label_cmap, ref_point=1/49)\n",
    "\n",
    "heatmap = go.Heatmap(z = rel_q100.T.values, x = rel_q100.index, y = rel_q100.columns, name = 'corr', \n",
    "                          colorscale=colorscl)\n",
    "layout = go.Layout(\n",
    "        title= 'Heatmap of relative likelihood of Cluster k being used in consumption quantile',\n",
    "        xaxis=dict(title = 'total daily demand quantiles (Amps) - log scale', type='log'),\n",
    "        yaxis=dict(title ='Cluster k'))\n",
    "fig = {'data':[heatmap], 'layout':layout }\n",
    "po.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Cluster Representativity and Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_consE, peak_consE, peak_coincR, temporal_entropy, demand_entropy = ec.getMeasures(best_exp, 1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumption Error - total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.subplotClusterMetrics(total_consE, 'TOTAL consumption error evaluation metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumption Error - max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.subplotClusterMetrics(peak_consE, 'PEAK consumption error evaluation metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak Coincidence Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterMetrics(peak_coincR, 'daily peak coincidence ratios', metric='coincidence_ratio', make_area_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Entropy - TEMPORAL\n",
    "#### weekday, month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterMetrics(temporal_entropy, 'time-derived cluster entropy')#, metric='weekday_entropy', make_area_plot=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Entropy - ENERGY DEMAND\n",
    "#### total daily demand, max daily demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterMetrics(demand_entropy, 'demand-based cluster entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Scoring Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec.saveMeasures(best_exp, 1200)\n",
    "data = pd.read_csv(os.path.join(eval_dir,'cluster_entropy.csv'), index_col=[0,1], header=[0,1,2])\n",
    "data.reset_index(level=0, drop=True, inplace=True)\n",
    "data.rename(dict(zip(data.index, [s.replace('_', ' ', 2) for s in data.index])),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_coincR = df[['coincR']].rank(ascending=False).groupby(level=['measure','metric'],axis=1).mean().T\n",
    "\n",
    "rank_consE = df[['consE']].rank().groupby(level=['measure'],axis=1).mean().T\n",
    "rank_consE.insert(loc=0, column='metric', value='mean_error')\n",
    "rank_consE.set_index('metric',append=True,inplace=True)\n",
    "\n",
    "rank_entropy = df['entropy'].rank().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['consE']].rank().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranked_results = pd.concat([rank_coincR, rank_consE, rank_entropy], levels=['measure','metric'])\n",
    "ranked_results.insert(loc=0, column='weights', value= [4, 7 ,7, 6, 6, 5, 5])#, 2])\n",
    "\n",
    "score_results = ranked_results.loc[:,ranked_results.columns[1::]].multiply(ranked_results['weights'], axis='index').sum()\n",
    "score = pd.DataFrame(score_results, columns=['score']).T\n",
    "score.index = pd.MultiIndex.from_tuples([('', '', 'SCORE')])\n",
    "\n",
    "ranked_results.set_index('weights',append=True,inplace=True)\n",
    "score_results = ranked_results.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterCentroids(real_cluster_centroids['exp8_kmeans_unit_norm'].loc[[40, 41, 43, 44, 45, 48, 49, 50, 53]], \n",
    "                        groupby=None, \n",
    "                        title='Customer Archetype: Rural Free State') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterCentroids(real_cluster_centroids['exp8_kmeans_unit_norm'].loc[[34, 40, 43, 44, 48, 49, 50, 53]], \n",
    "                        groupby=None, \n",
    "                        title='Customer Archetype: Informal Settlement Newly Electrified') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterCentroids(real_cluster_centroids['exp8_kmeans_unit_norm'].loc[[3, 4, 23, 25, 27, 28, 29]], \n",
    "                        groupby=None, \n",
    "                        title='Customer Archetype: Township KZN Longterm Electrified') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterCentroids(real_cluster_centroids['exp8_kmeans_unit_norm'].loc[[2, 4, 6, 7, 31, 36, 37, 38]], \n",
    "                        groupby=None, \n",
    "                        title='Customer Archetype: Upper Middle Class Longterm Electrified') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
