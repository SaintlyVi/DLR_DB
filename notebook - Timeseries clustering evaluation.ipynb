{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Load Profile Timeseries Clustering Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "from math import ceil\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.offline as po\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tools\n",
    "import colorlover as cl\n",
    "#from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "\n",
    "import matplotlib. pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import evaluation.eval_clusters as ec\n",
    "import evaluation.eval_cluster_plot as pc\n",
    "from support import data_dir\n",
    "eval_dir = os.path.join(data_dir,'cluster_evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = ec.getExperiments('exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Cluster Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davies-Bouldin Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterIndex('dbi', 'Davies-Bouldin Index', experiments, groupby='algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Index Adequacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterIndex('mia','Mean Index Adequacy', experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score\n",
    "\n",
    "The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pc.plotClusterIndex('silhouette', 'Silhouette Score', experiments, groupby='algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Cluster Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterIndex('score','Combined Cluster Score', experiments, groupby='algorithm', ylog=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Cluster Labels, Centroids and Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select best clusters for different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_results = ec.readResults()\n",
    "\n",
    "selected_clusters = dict()\n",
    "for e in experiments:\n",
    "    clusters = ec.selectClusters(cluster_results, 5, experiment=e)\n",
    "    selected_clusters[e] = clusters\n",
    "selected_clusters['best'] = ec.selectClusters(cluster_results, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "experiment_rank = selected_clusters['best'].set_axis(range(1,11), inplace=False)\n",
    "experiment_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_exp = ['exp2_kmeans_unit_norm', 'exp5_kmeans_zero-one', 'exp4_kmeans_zero-one', 'exp6_kmeans_unit_norm',\n",
    "            'exp5_kmeans_unit_norm','exp7_kmeans_unit_norm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get denormalised (real) cluster centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_cluster_centroids = dict()\n",
    "\n",
    "for e in best_exp:\n",
    "    rccpath = os.path.join(eval_dir, 'best_centroids', e +'BEST1_centroids.csv')\n",
    "    centroids  = pd.read_csv(rccpath, index_col='k')\n",
    "    real_cluster_centroids[e] = centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "ec.exploreAMDBins(best_exp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    pc.plotClusterCentroids(real_cluster_centroids[best_exp[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Patterns in Cluster Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise TEMPORAL Cluster Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 5\n",
    "pc.plotClusterSpecificity(best_exp[i], corr_list=['daytype','weekday'], threshold=1200, relative=[[5,1,1],1])\n",
    "pc.plotClusterSpecificity(best_exp[i], corr_list=['season','monthly'], threshold=1200, relative=[[8, 4],1])\n",
    "pc.plotClusterSpecificity(best_exp[i], corr_list=['yearly'], threshold=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise CONTEXTUAL Cluster Specificity (Daily Demand Assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment = 'exp7_kmeans_unit_norm'\n",
    "\n",
    "corr_path = os.path.join(data_dir, 'cluster_evaluation', 'k_correlations')\n",
    "\n",
    "dif = pd.read_csv(os.path.join(corr_path, 'demandi_corr.csv'), index_col=[0,1,2], header=[0]).drop_duplicates()\n",
    "dif_temp = dif.reset_index(level=[-2,-1])\n",
    "int100_total = dif_temp[(dif_temp.experiment==experiment+'BEST1')&(dif_temp.compare=='total')].drop(['experiment','compare'],axis=1)\n",
    "\n",
    "dqf = pd.read_csv(os.path.join(corr_path, 'demandq_corr.csv'), index_col=[0,1,2], header=[0]).drop_duplicates()\n",
    "dqf_temp = dqf.reset_index(level=[-2,-1])\n",
    "q100_total = dqf_temp[(dqf_temp.experiment==experiment+'BEST1')&(dqf_temp.compare=='total')].drop(['experiment','compare'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Equally spaced daily demand intervals\n",
    "i = int100_total.T.stack().reset_index()\n",
    "i.columns = ['int100_bins', 'cluster', 'values']\n",
    "\n",
    "heatmap = go.Heatmap(z = i['values'], x = i['int100_bins'], y = i['cluster'], \n",
    "                          colorscale='Reds')\n",
    "layout = go.Layout(\n",
    "        title= 'Relative likelihood that cluster k is used in particular consumption bin',\n",
    "        xaxis=dict(title = 'total daily demand bins (Amps)', \n",
    "                   tickmode='array', tickvals=list(range(0,100,10)), ticktext = list(range(0,1000,100))),\n",
    "        yaxis=dict(title ='k clusters for exp7_kmeans_unit_norm')\n",
    "        )\n",
    "fig = {'data':[heatmap], 'layout':layout }\n",
    "po.iplot(fig)\n",
    "\n",
    "#Equally sized daily demand intervals (quantiles)\n",
    "rel_q100 = q100_total.T[1::]#.drop(columns=37)/0.01\n",
    "\n",
    "slatered=['#232c2e', '#ffffe0','#c34513']\n",
    "label_cmap, label_cs = pc.colorscale_from_list(slatered, 'label_cmap') \n",
    "colorscl= pc.asymmetric_colorscale(rel_q100, label_cmap, ref_point=1/49)\n",
    "\n",
    "heatmap = go.Heatmap(z = rel_q100.T.values, x = rel_q100.index, y = rel_q100.columns, name = 'corr', \n",
    "                          colorscale=colorscl)\n",
    "layout = go.Layout(\n",
    "        title= 'Heatmap of relative likelihood of Cluster k being used in consumption quantile',\n",
    "        xaxis=dict(title = 'total daily demand quantiles (Amps) - log scale', type='log'),\n",
    "        yaxis=dict(title ='Cluster k'))\n",
    "fig = {'data':[heatmap], 'layout':layout }\n",
    "po.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Cluster Representativity and Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_consE, peak_consE, peak_coincR, temporal_entropy, demand_entropy = ec.getMeasures(best_exp, 1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumption Error - total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterMetrics(total_consE, 'TOTAL consumption error evaluation metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumption Error - max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterMetrics(peak_consE, 'PEAK consumption error evaluation metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak Coincidence Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterMetrics(peak_coincR, 'daily peak coincidence ratios', metric='coincidence_ratio', make_area_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Entropy - TEMPORAL\n",
    "#### weekday, month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterMetrics(temporal_entropy, 'time-derived cluster entropy')#, metric='weekday_entropy', make_area_plot=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Entropy - ENERGY DEMAND\n",
    "#### total daily demand, max daily demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterMetrics(demand_entropy, 'demand-based cluster entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ec.saveMeasures(best_exp, 1200)\n",
    "df = pd.read_csv(os.path.join(eval_dir,'cluster_entropy.csv'), index_col=[0,1], header=[0,1,2])\n",
    "df.reset_index(level=0, drop=True, inplace=True)\n",
    "df.rename(dict(zip(df.index, [s.replace('_', ' ', 2) for s in df.index])),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_coincR = df[['coincR']].rank(ascending=False).groupby(level=['measure','metric'],axis=1).mean().T\n",
    "\n",
    "rank_consE = df[['consE']].rank().groupby(level=['measure'],axis=1).mean().T\n",
    "rank_consE.insert(loc=0, column='metric', value='mean_error')\n",
    "rank_consE.set_index('metric',append=True,inplace=True)\n",
    "\n",
    "rank_entropy = df['entropy'].rank().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranked_results = pd.concat([rank_coincR, rank_consE, rank_entropy], levels=['measure','metric'])\n",
    "ranked_results.insert(loc=0, column='weights', value= [4, 7 ,7, 6, 6, 5, 5, 2])\n",
    "\n",
    "score_results = ranked_results.loc[:,ranked_results.columns[1::]].multiply(ranked_results['weights'], axis='index').sum()\n",
    "score = pd.DataFrame(score_results, columns=['score']).T\n",
    "score.index = pd.MultiIndex.from_tuples([('', '', 'SCORE')])\n",
    "\n",
    "ranked_results.set_index('weights',append=True,inplace=True)\n",
    "score_results = ranked_results.append(score)\n",
    "score_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
