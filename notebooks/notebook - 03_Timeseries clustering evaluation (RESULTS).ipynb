{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Load Profile Timeseries Clustering Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "from math import ceil, log\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.offline as po\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.tools as tools\n",
    "import colorlover as cl\n",
    "#from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "\n",
    "import matplotlib. pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import evaluation.eval_clusters as ec\n",
    "import evaluation.eval_cluster_plot as pc\n",
    "from support import data_dir, image_dir, results_dir\n",
    "eval_dir = os.path.join(data_dir,'cluster_evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = ec.getExperiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_exp = ['exp2_kmeans_unit_norm', 'exp4_kmeans_zero-one', 'exp5_kmeans_unit_norm', 'exp5_kmeans_zero-one', \n",
    "            'exp6_kmeans_unit_norm','exp7_kmeans_unit_norm','exp8_kmeans_unit_norm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Cluster Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davies-Bouldin Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterIndex('dbi', 'Davies-Bouldin Index', experiments, groupby='algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Index Adequacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterIndex('mia','Mean Index Adequacy', experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score\n",
    "\n",
    "The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterIndex('silhouette', 'Silhouette Score', experiments, groupby='experiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Cluster Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterIndex('score','Ix Score for all Experiments', experiments, groupby='algorithm', ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterIndex('score','Combined Index Score', experiments, groupby='experiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Cluster Labels, Centroids and Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select best clusters for different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_results = ec.readResults()\n",
    "selected_clusters = ec.selectClusters(cluster_results, len(cluster_results))\n",
    "selected_clusters.rename(columns={'experiment':'Experiment','algorithm':'Algorithm','preprocessing':'Norm', \n",
    "                                  'SOM dimensions':'SOM dim','clusters':'Clusters','dbi':'DBI', 'mia':'MIA', \n",
    "                                  'silhouette':'Silhouette','score':'CI score','run time':'Run time',\n",
    "                                  'experiment_name':'Experiment name'}, inplace=True)\n",
    "top10 = selected_clusters.round({'DBI':4, 'MIA':4, 'Silhouette':4, 'CI score': 6, 'Run time':2}).head(10).set_axis(range(1,11), inplace=False)\n",
    "top10.reset_index().rename(columns={'index':'Rank'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentage experiments with CI score below 4\n",
    "ci4 = selected_clusters.loc[selected_clusters['CI score']<4,'CI score'].count()/len(selected_clusters) \n",
    "\n",
    "#Percentage experiments with CI score below 6.5\n",
    "ci65 = selected_clusters.loc[selected_clusters['CI score']<6.5,'CI score'].count()/len(selected_clusters) \n",
    "\n",
    "#Max CI score\n",
    "cimax = selected_clusters['CI score'].max()\n",
    "\n",
    "#Score difference between best and tenth best experiment\n",
    "(top10.iloc[9,8] - top10.iloc[0,8])/top10.iloc[0,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms of algorithm performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = [go.Histogram(x=selected_clusters['CI score'], nbinsx = 200, histnorm='percent')]\n",
    "layout = dict(title='Distribution of CI Scores across Clustering Algorithms', titlefont=dict(size=20),\n",
    "                     xaxis = dict(title='CI score bins', titlefont=dict(size=16), tickfont=dict(size=16)), \n",
    "                     yaxis = dict(title='Percent', titlefont=dict(size=16), tickfont=dict(size=16)),\n",
    "                     margin=dict(t=30, l=40, b=40),\n",
    "                     height=350, width=1000)\n",
    "# Plot!\n",
    "fig0 = go.Figure(data=data, layout=layout)\n",
    "po.iplot(fig0)\n",
    "#po.plot(fig0, filename=data_dir+'/cluster_evaluation/plots/clustering_evaluation/DistplotQuantScoresAll'+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = selected_clusters[selected_clusters.Norm.isna()]['CI score']  \n",
    "x1 = selected_clusters[selected_clusters.Norm=='unit_norm']['CI score']  \n",
    "x2 = selected_clusters[selected_clusters.Norm=='demin']['CI score']\n",
    "x3 = selected_clusters[selected_clusters.Norm=='zero-one']['CI score'] \n",
    "x4 = selected_clusters[selected_clusters.Norm=='sa_norm']['CI score']\n",
    "\n",
    "# Group data together\n",
    "hist_data = [x0, x1, x2, x3, x4]\n",
    "\n",
    "group_labels = ['no norm', 'unit norm', 'demin', 'zero-one', 'SA norm']\n",
    "\n",
    "# Create distplot with custom bin_size\n",
    "fig = ff.create_distplot(hist_data, group_labels, histnorm='percent', bin_size=0.05, \n",
    "                         show_curve=False, show_rug=False)\n",
    "fig['layout'].update(title='Distribution of Quantitative Scores across Normalisation Algorithms', titlefont=dict(size=16),\n",
    "                     xaxis = dict(title='CI score bins'), \n",
    "                     yaxis = dict(title='Percent'),\n",
    "                     margin=dict(t=30, l=30, b=30),\n",
    "                     height=250, width=600)\n",
    "\n",
    "# Plot!\n",
    "po.iplot(fig)\n",
    "po.plot(fig, filename=data_dir+'/cluster_evaluation/plots/clustering_evaluation/DistplotQuantScoresNormalisation'+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = selected_clusters[selected_clusters['Experiment name'].str.contains('exp1|exp2|exp3')]['CI score']  \n",
    "y1 = selected_clusters[selected_clusters['Experiment name'].str.contains('exp4|exp5|exp6')]['CI score']  \n",
    "y2 = selected_clusters[selected_clusters['Experiment name'].str.contains('exp7|exp8')]['CI score']\n",
    "\n",
    "# Group data together\n",
    "hist_data2 = [y0, y1, y2]\n",
    "\n",
    "group_labels2 = ['no pre-binning', 'AMC', 'integral kmeans']\n",
    "\n",
    "fig2 = ff.create_distplot(hist_data2, group_labels2, histnorm='percent', bin_size=0.05, \n",
    "                          show_curve=False, show_rug=False, colors=['#393E46', '#2BCDC1', '#F66095'])\n",
    "fig2['layout'].update(title='Distribution of Quantitative Scores across Pre-binning Algorithms', titlefont=dict(size=16),\n",
    "                     xaxis = dict(title='CI score bins'), \n",
    "                     yaxis = dict(title='Percent'),\n",
    "                     margin=dict(t=30, l=30, b=30),\n",
    "                     height=250, width=600)\n",
    "# Plot!\n",
    "po.iplot(fig2)\n",
    "po.plot(fig2, filename=data_dir+'/cluster_evaluation/plots/clustering_evaluation/DistplotQuantScoresPrebinning'+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z0 = selected_clusters[selected_clusters.Algorithm=='kmeans']['CI score']  \n",
    "z1 = selected_clusters[selected_clusters.Algorithm=='som']['CI score']  \n",
    "z2 = selected_clusters[selected_clusters.Algorithm=='som+kmeans']['CI score']\n",
    "\n",
    "# Group data together\n",
    "hist_data3 = [z0, z1, z2]\n",
    "\n",
    "group_labels3 = ['kmeans', 'som', 'som+kmeans']\n",
    "\n",
    "fig3 = ff.create_distplot(hist_data3, group_labels3, histnorm='percent', bin_size=0.05, \n",
    "                          show_curve=False, show_rug=False, colors=['#1E90FF','#DC143C', '#800080'])\n",
    "fig3['layout'].update(title='Distribution of Quantitative Scores across Clustering Algorithms', titlefont=dict(size=16),\n",
    "                     xaxis = dict(title='CI score bins'), \n",
    "                     yaxis = dict(title='Percent'),\n",
    "                     margin=dict(t=30, l=30, b=30),\n",
    "                     height=250, width=600)\n",
    "# Plot!\n",
    "po.iplot(fig3)\n",
    "po.plot(fig3, filename=data_dir+'/cluster_evaluation/plots/clustering_evaluation/DistplotQuantScoresClustering'+'.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse algorithm run times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimes = selected_clusters.loc[(selected_clusters.Norm=='unit_norm')].groupby('Algorithm')[['CI score','Run time']].mean()\n",
    "runtimes.rename(columns={'Run time':'Mean run time (s)','CI score':'Mean CI score'}, inplace=True)\n",
    "runtimes.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kmeansruntimes = selected_clusters.loc[(selected_clusters.Algorithm=='kmeans')].groupby('Clusters')['Run time'].mean()\n",
    "somkmeansruntimes = selected_clusters.loc[(selected_clusters.Algorithm=='som+kmeans')].groupby('SOM dim')['Run time'].mean()\n",
    "somruntimes = selected_clusters.loc[(selected_clusters.Algorithm=='som')].groupby('SOM dim')['Run time'].mean()\n",
    "\n",
    "data = [go.Scatter(x=somruntimes.index**2,\n",
    "                  y=somruntimes.values,\n",
    "                  name='som',\n",
    "                  mode='lines'),\n",
    "        go.Scatter(x=kmeansruntimes.index,\n",
    "                  y=kmeansruntimes.values,\n",
    "                  name='k-means',\n",
    "                  mode='lines')\n",
    "       ]\n",
    "layout = dict(title='Run times for som and k-means algorithms', titlefont=dict(size=18),\n",
    "                     xaxis = dict(title='number of SOM dimensions or clusters', titlefont=dict(size=16), tickfont=dict(size=16)), \n",
    "                     yaxis = dict(title='run time (s)', titlefont=dict(size=16), tickfont=dict(size=16)),\n",
    "                     margin=dict(t=30),\n",
    "                     height=350, width=600)\n",
    "# Plot!\n",
    "fig0 = go.Figure(data=data, layout=layout)\n",
    "po.iplot(fig0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get denormalised (real) cluster centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_cluster_centroids = dict()\n",
    "\n",
    "for e in best_exp:\n",
    "    rccpath = os.path.join(eval_dir, 'best_centroids', e +'BEST1_centroids.csv')\n",
    "    centroids  = pd.read_csv(rccpath, index_col='k')\n",
    "    real_cluster_centroids[e] = centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 6\n",
    "ex = ec.exploreAMDBins(best_exp[i]).reset_index()\n",
    "mapper = ec.mapBins(real_cluster_centroids[best_exp[i]])\n",
    "out = pd.merge(ex, mapper, on='elec_bin').sort_values(by='mean_dd')\n",
    "out.rename(columns={'total_sample':'Members','score':'Ix','n_clust':'Clusters','bin_labels':'Mean daily demand bin'}, inplace=True)\n",
    "out = out.round({'Ix':3})\n",
    "o = out.reset_index().drop(columns=['som_dim','elec_bin','mean_dd','index'],axis=0)\n",
    "po = o.pivot(index=o.index, columns='experiment_name').swaplevel(axis=1)\n",
    "po.set_index((best_exp[i], 'Mean daily demand bin'), inplace=True)\n",
    "po.index.rename('Mean daily demand bin', inplace=True)\n",
    "po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,7):\n",
    "    pc.plotClusterCentroids(real_cluster_centroids[best_exp[i]])#, threshold=10490, groupby=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Centroid and Member Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "centroids = ec.realCentroids(best_exp[i])\n",
    "centroids['cluster_size'].plot('bar', figsize=(14,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = centroids.nlargest(15, 'cluster_size').sort_index().index.values\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pc.plotMembersSample(best_exp[i], largest=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Patterns in Cluster Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise TEMPORAL Cluster Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,7):\n",
    "    pc.plotClusterSpecificity(best_exp[i], corr_list=['daytype','weekday'], threshold=10490, relative=[[5,1,1],1])\n",
    "    pc.plotClusterSpecificity(best_exp[i], corr_list=['season','monthly'], threshold=10490, relative=[[8, 4],1])\n",
    "    pc.plotClusterSpecificity(best_exp[i], corr_list=['yearly'], threshold=10490)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise CONTEXTUAL Cluster Specificity (Daily Demand Assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment = 'exp8_kmeans_unit_norm'\n",
    "\n",
    "corr_path = os.path.join(data_dir, 'cluster_evaluation', 'k_correlations')\n",
    "\n",
    "dif = pd.read_csv(os.path.join(corr_path, 'demandi_corr.csv'), index_col=[0,1,2], header=[0]).drop_duplicates()\n",
    "dif_temp = dif.reset_index(level=[-2,-1])\n",
    "int100_total = dif_temp[(dif_temp.experiment==experiment+'BEST1')&(dif_temp.compare=='total')].drop(['experiment','compare'],axis=1)\n",
    "\n",
    "dqf = pd.read_csv(os.path.join(corr_path, 'demandq_corr.csv'), index_col=[0,1,2], header=[0]).drop_duplicates()\n",
    "dqf_temp = dqf.reset_index(level=[-2,-1])\n",
    "q100_total = dqf_temp[(dqf_temp.experiment==experiment+'BEST1')&(dqf_temp.compare=='total')].drop(['experiment','compare'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Equally spaced daily demand intervals\n",
    "i = int100_total.T.stack().reset_index()\n",
    "i.columns = ['int100_bins', 'cluster', 'values']\n",
    "\n",
    "heatmap = go.Heatmap(z = i['values'], x = i['int100_bins'], y = i['cluster'], \n",
    "                          colorscale='Reds')\n",
    "layout = go.Layout(\n",
    "        title= 'Relative likelihood that cluster k is used in particular consumption bin',\n",
    "        xaxis=dict(title = 'total daily demand bins (Amps)', \n",
    "                   tickmode='array', tickvals=list(range(0,100,10)), ticktext = list(range(0,1000,100))),\n",
    "        yaxis=dict(title ='k clusters for '+experiment)\n",
    "        )\n",
    "fig = {'data':[heatmap], 'layout':layout }\n",
    "po.iplot(fig)\n",
    "\n",
    "#Equally sized daily demand intervals (quantiles)\n",
    "rel_q100 = q100_total.T[1::]#.drop(columns=37)/0.01\n",
    "\n",
    "slatered=['#232c2e', '#ffffe0','#c34513']\n",
    "label_cmap, label_cs = pc.colorscale_from_list(slatered, 'label_cmap') \n",
    "colorscl= pc.asymmetric_colorscale(rel_q100, label_cmap, ref_point=1/49)\n",
    "\n",
    "heatmap = go.Heatmap(z = rel_q100.T.values, x = rel_q100.index, y = rel_q100.columns, name = 'corr', \n",
    "                          colorscale=colorscl)\n",
    "layout = go.Layout(\n",
    "        title= 'Heatmap of relative likelihood of Cluster k being used in consumption quantile',\n",
    "        xaxis=dict(title = 'total daily demand quantiles (Amps) - log scale', type='log'),\n",
    "        yaxis=dict(title ='Cluster k'))\n",
    "fig = {'data':[heatmap], 'layout':layout }\n",
    "po.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Cluster Representativity and Homogeneity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_consE, peak_consE, peak_coincR, temporal_entropy, demand_entropy, good_clusters = ec.getMeasures(best_exp, \n",
    "                                                                                                       threshold = 10490,\n",
    "                                                                                                       weighted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumption Error - total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pc.subplotClusterMetrics(total_consE, 'TOTAL consumption error evaluation metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumption Error - max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.subplotClusterMetrics(peak_consE, 'PEAK consumption error evaluation metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak Coincidence Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterMetrics(peak_coincR, 'daily peak coincidence ratios', metric='coincidence_ratio', make_area_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Entropy - TEMPORAL\n",
    "#### weekday, month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterMetrics(temporal_entropy, 'weekday cluster entropy', metric='weekday_entropy')#, make_area_plot=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterMetrics(temporal_entropy, 'monthly cluster entropy', metric='monthly_entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Entropy - ENERGY DEMAND\n",
    "#### total daily demand, max daily demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterMetrics(demand_entropy, 'total demand cluster entropy', metric='total_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterMetrics(demand_entropy, 'peak demand cluster entropy', metric='peak_entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Scoring Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec.saveMeasures(best_exp, 10490, weighted=True)\n",
    "data = pd.read_csv(os.path.join(eval_dir,'cluster_entropy.csv'), index_col=[0,1], header=[0,1,2])\n",
    "data.reset_index(level=0, drop=True, inplace=True)\n",
    "data.rename(dict(zip(data.index, [s.replace('_', ' ', 2) for s in data.index])),inplace=True)\n",
    "df = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unweighted Mean Peak Coincidence Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myd = pd.DataFrame()\n",
    "for x in peak_coincR.keys(): #set threshold value to same as data - 10490\n",
    "    myd = myd.append({'experiment': x.replace('_',' ', 2), 'mean peak coincidence ratio': peak_coincR[x]['coincidence_ratio'].mean()}, ignore_index=True)\n",
    "#myd = myd.set_index('experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrr = df.loc(axis=1)[:,:,'coincidence_ratio']\n",
    "rrr.columns = rrr.columns.droplevel().droplevel()\n",
    "rrr.reset_index(inplace=True)\n",
    "pcr = pd.merge(myd, rrr, left_on='experiment', right_on='index')\n",
    "pcr.rename(columns={'mean peak coincidence ratio':'Mean pcr','coincidence_ratio':'Weighted pcr',\n",
    "                    'experiment':'Experiment'},inplace=True)\n",
    "pcr.set_index('Experiment',inplace=True)\n",
    "pcr.drop(columns=['index'],inplace=True)\n",
    "pcr.round(3).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranked Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_coincR = df[['coincR']].rank(ascending=False, method='min').groupby(level=['measure','metric'],axis=1).mean().T\n",
    "rank_clusters = df[['clusters']].rank(ascending=False, method='min').groupby(level=['measure','metric'],axis=1).mean().T\n",
    "\n",
    "rank_consE = df[['consE']].rank(method='min').groupby(level=['measure'],axis=1).mean().T\n",
    "rank_consE.insert(loc=0, column='metric', value='mean_error')\n",
    "rank_consE.set_index('metric',append=True,inplace=True)\n",
    "\n",
    "rank_entropy = df['entropy'].rank(method='min').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conse = df[['consE']].rank(method='min').T\n",
    "conse.rename(columns={'experiment':'Experiment','algorithm':'Algorithm','preprocessing':'Norm', \n",
    "                                  'SOM dimensions':'SOM dim','clusters':'Clusters','dbi':'DBI', 'mia':'MIA', \n",
    "                                  'silhouette':'Silhouette','score':'CI score','run time':'Run time',\n",
    "                                  'experiment_name':'Experiment name'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranked_results = pd.concat([rank_clusters, rank_coincR, rank_consE, rank_entropy], levels=['measure','metric'])\n",
    "ranked_results.insert(loc=0, column='weights', value= [2, 3, 6 ,6, 5, 5, 4, 4])#, 2])\n",
    "\n",
    "score_results = ranked_results.loc[:,ranked_results.columns[1::]].multiply(ranked_results['weights'], axis='index').sum()\n",
    "score = pd.DataFrame(score_results, columns=['score']).T\n",
    "score.index = pd.MultiIndex.from_tuples([('', '', 'SCORE')])\n",
    "\n",
    "ranked_results.set_index('weights',append=True,inplace=True)\n",
    "score_results = ranked_results.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run this cell if you want information about additional parameters for experiments\n",
    "algs = [col.split(' ') for col in score_results.columns]\n",
    "preb = ['','AMC','AMC','AMC','AMC','integral k-means','integral k-means']\n",
    "dropz = ['','','','','True','','True']\n",
    "multic = []\n",
    "for a in range(0, len(algs)):\n",
    "    multic.append(algs[a]+[preb[a]]+[dropz[a]])\n",
    "score_results.columns = pd.MultiIndex.from_tuples(multic, names=['Experiment', 'Algorithm','Normalisation',\n",
    "                                                                 'Pre-binning','Drop Zeros'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_results.index.set_names('weight', level=2, inplace=True)\n",
    "score_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterCentroids(real_cluster_centroids['exp8_kmeans_unit_norm'].loc[[33, 39, 40, 41, 44, 45, 46, 47, 48, 49, 50, 51]], \n",
    "                        groupby=None,\n",
    "                        title='Mpumalanga Rural Newly Electrified', threshold=10490) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterCentroids(real_cluster_centroids['exp8_kmeans_unit_norm'].loc[[39, 44, 45, 46, 49, 50]], \n",
    "                        groupby=None, \n",
    "                        title='Mpumalanga Informal Settlement Newly Electrified', threshold=10490) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterCentroids(real_cluster_centroids['exp8_kmeans_unit_norm'].loc[[39, 45, 46, 49, 50, 53]], \n",
    "                        groupby=None, \n",
    "                        title='Eastern Cape Informal Settlement Newly Electrified', threshold=10490) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterCentroids(real_cluster_centroids['exp8_kmeans_unit_norm'].loc[[9, 11, 44]], \n",
    "                        groupby=None, \n",
    "                        title='Limpopo Informal Settlement Medium-term Electrified', threshold=10490) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterCentroids(real_cluster_centroids['exp8_kmeans_unit_norm'].loc[[3, 4, 6, 7, 24]], \n",
    "                        groupby=None, \n",
    "                        title='Gauteng Township Longterm Electrified', threshold=10490) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterCentroids(real_cluster_centroids['exp8_kmeans_unit_norm'].loc[[1, 3, 4, 5, 35, 36, 38]], \n",
    "                        groupby=None, \n",
    "                        title='KwaZulu Natal Lower Middle Class Long-term Electrified', threshold=10490) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterCentroids(real_cluster_centroids['exp8_kmeans_unit_norm'].loc[[2, 4, 35, 36, 38, 57]], \n",
    "                        groupby=None, \n",
    "                        title='KwaZulu Natal Upper Middle Class Long-term Electrified', threshold=10490) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plotClusterCentroids(real_cluster_centroids['exp8_kmeans_unit_norm'].loc[[6, 7, 37, 54, 57]], \n",
    "                        groupby=None, \n",
    "                        title='Western Cape Upper Middle Class Medium-term Electrified', threshold=10490) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
